# Nightly Integration Tests for WASP2
# Runs extended test suite, benchmarks, and cross-platform validation
# Based on best practices from GenVarLoader, uv, and pysam projects

name: Nightly

on:
  schedule:
    # Daily at 3am UTC (after Dependabot updates)
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      test_set:
        description: 'Test set to run'
        type: choice
        default: 'all'
        options:
          - all
          - unit
          - integration
          - benchmarks
          - nextflow

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1
  PYTHONDONTWRITEBYTECODE: 1

jobs:
  # Planning job - determines what to run
  plan:
    name: Plan Tests
    runs-on: ubuntu-latest
    outputs:
      run_unit: ${{ steps.plan.outputs.run_unit }}
      run_integration: ${{ steps.plan.outputs.run_integration }}
      run_benchmarks: ${{ steps.plan.outputs.run_benchmarks }}
      run_nextflow: ${{ steps.plan.outputs.run_nextflow }}
    steps:
      - name: Determine test plan
        id: plan
        env:
          INPUT_TEST_SET: ${{ github.event.inputs.test_set }}
        run: |
          TEST_SET="${INPUT_TEST_SET:-all}"
          if [[ "$TEST_SET" == "all" ]]; then
            echo "run_unit=true" >> $GITHUB_OUTPUT
            echo "run_integration=true" >> $GITHUB_OUTPUT
            echo "run_benchmarks=true" >> $GITHUB_OUTPUT
            echo "run_nextflow=true" >> $GITHUB_OUTPUT
          else
            echo "run_unit=$([[ "$TEST_SET" == "unit" ]] && echo true || echo false)" >> $GITHUB_OUTPUT
            echo "run_integration=$([[ "$TEST_SET" == "integration" ]] && echo true || echo false)" >> $GITHUB_OUTPUT
            echo "run_benchmarks=$([[ "$TEST_SET" == "benchmarks" ]] && echo true || echo false)" >> $GITHUB_OUTPUT
            echo "run_nextflow=$([[ "$TEST_SET" == "nextflow" ]] && echo true || echo false)" >> $GITHUB_OUTPUT
          fi

  # Extended unit tests with coverage
  unit-tests:
    name: Extended Unit Tests
    needs: plan
    if: needs.plan.outputs.run_unit == 'true'
    runs-on: [self-hosted, macOS, ARM64, python, testing]
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.10', '3.11', '3.12']

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        env:
          PYTHON_VERSION: ${{ matrix.python-version }}
        run: |
          echo "Using Python $PYTHON_VERSION"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-nightly-${{ matrix.python-version }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            pip-nightly-${{ matrix.python-version }}-
            pip-nightly-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip --quiet
          pip install maturin pytest pytest-cov pytest-xdist --quiet

      - name: Build Rust extension
        run: |
          maturin build --release -m rust/Cargo.toml
          pip install rust/target/wheels/*.whl --force-reinstall --quiet

      - name: Install package
        run: pip install -e ".[dev]" --no-build-isolation --quiet || pip install -e "." --quiet

      - name: Run unit tests with coverage
        run: |
          pytest tests/ -v --tb=short \
            --cov=src --cov-report=xml --cov-report=html \
            -m "not slow and not benchmark and not integration" \
            -n auto

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage.xml
          flags: nightly-py${{ matrix.python-version }}
          fail_ci_if_error: false

  # Integration tests
  integration-tests:
    name: Integration Tests
    needs: plan
    if: needs.plan.outputs.run_integration == 'true'
    runs-on: [self-hosted, macOS, ARM64, analysis, bioinformatics]
    timeout-minutes: 60

    steps:
      - uses: actions/checkout@v4

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-integration-${{ hashFiles('pyproject.toml') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip --quiet
          pip install maturin pytest --quiet

      - name: Build and install
        run: |
          maturin build --release -m rust/Cargo.toml
          pip install rust/target/wheels/*.whl --force-reinstall --quiet
          pip install -e ".[dev]" --no-build-isolation --quiet || pip install -e "." --quiet

      - name: Run integration tests
        run: |
          pytest tests/ -v --tb=short -m "integration" -s

      - name: Verify CLI tools
        run: |
          wasp2-count --help
          wasp2-map --help
          wasp2-analyze --help
          python -c "import wasp2_rust; print('Rust extension OK')"

  # Performance benchmarks
  benchmarks:
    name: Performance Benchmarks
    needs: plan
    if: needs.plan.outputs.run_benchmarks == 'true'
    runs-on: [self-hosted, macOS, ARM64, analysis, bioinformatics]
    timeout-minutes: 90

    steps:
      - uses: actions/checkout@v4

      - name: Cache benchmark data
        uses: actions/cache@v4
        with:
          path: |
            tests/data/benchmark_cache/
            ~/.cache/wasp2/
          key: benchmark-data-${{ hashFiles('tests/benchmarks/**') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip --quiet
          pip install maturin pytest pytest-benchmark --quiet

      - name: Build optimized
        run: |
          maturin build --release -m rust/Cargo.toml
          pip install rust/target/wheels/*.whl --force-reinstall --quiet
          pip install -e ".[dev]" --no-build-isolation --quiet || pip install -e "." --quiet

      - name: Run benchmarks
        run: |
          pytest tests/ -v --tb=short -m "benchmark" \
            --benchmark-only \
            --benchmark-autosave \
            --benchmark-compare \
            --benchmark-json=benchmark-results.json || true

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_id }}
          path: |
            .benchmarks/
            benchmark-results.json

  # Nextflow pipeline tests (optional, expensive)
  nextflow-tests:
    name: Nextflow Pipeline Tests
    needs: plan
    if: needs.plan.outputs.run_nextflow == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 120
    continue-on-error: true

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Set up Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache Rust
        uses: Swatinem/rust-cache@v2
        with:
          workspaces: rust

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libhts-dev libbz2-dev liblzma-dev zlib1g-dev \
            samtools bcftools tabix

      - name: Install Nextflow
        run: |
          curl -fsSL https://get.nextflow.io | bash
          sudo mv nextflow /usr/local/bin/

      - name: Build WASP2
        run: |
          pip install maturin
          maturin build --release -m rust/Cargo.toml
          pip install rust/target/wheels/*.whl
          pip install -e ".[dev]" --no-build-isolation || pip install -e "."

      - name: Run Nextflow tests
        working-directory: pipelines/nf-rnaseq
        run: |
          nextflow run . -profile test --outdir test_output || true
        env:
          NXF_ANSI_LOG: false

      - name: Upload Nextflow logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: nextflow-logs
          path: |
            pipelines/nf-rnaseq/.nextflow.log
            pipelines/nf-rnaseq/test_output/

  # Summary job
  nightly-summary:
    name: Nightly Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, benchmarks, nextflow-tests]
    if: always()

    steps:
      - name: Check results
        env:
          UNIT_RESULT: ${{ needs.unit-tests.result }}
          INTEGRATION_RESULT: ${{ needs.integration-tests.result }}
          BENCHMARKS_RESULT: ${{ needs.benchmarks.result }}
          NEXTFLOW_RESULT: ${{ needs.nextflow-tests.result }}
        run: |
          echo "## Nightly Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Suite | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Unit Tests | $UNIT_RESULT |" >> $GITHUB_STEP_SUMMARY
          echo "| Integration | $INTEGRATION_RESULT |" >> $GITHUB_STEP_SUMMARY
          echo "| Benchmarks | $BENCHMARKS_RESULT |" >> $GITHUB_STEP_SUMMARY
          echo "| Nextflow | $NEXTFLOW_RESULT |" >> $GITHUB_STEP_SUMMARY

      - name: Fail if critical tests failed
        env:
          UNIT_RESULT: ${{ needs.unit-tests.result }}
          INTEGRATION_RESULT: ${{ needs.integration-tests.result }}
        run: |
          if [[ "$UNIT_RESULT" == "failure" ]] || [[ "$INTEGRATION_RESULT" == "failure" ]]; then
            exit 1
          fi
