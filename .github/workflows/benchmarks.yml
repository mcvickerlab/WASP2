# WASP2 Benchmark Workflow
# Runs reproducible benchmarks and stores results as artifacts
# Validates performance claims: 61x faster filtering, 6.4x faster counting, r² > 0.99

name: Benchmarks

on:
  # Run on demand
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: true
        default: 'quick'
        type: choice
        options:
          - quick
          - full
          - counting
          - mapping
          - concordance

  # Run on releases
  release:
    types: [published]

  # Run weekly to track performance
  schedule:
    - cron: '0 6 * * 1'  # Monday at 6am UTC

permissions:
  contents: read

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  benchmark:
    name: Run Benchmarks
    runs-on: [self-hosted, macOS, ARM64, docker, wasp2]
    timeout-minutes: 60
    permissions:
      contents: read
    defaults:
      run:
        shell: bash

    env:
      BENCHMARK_TYPE: ${{ github.event.inputs.benchmark_type || 'quick' }}
      COMMIT_SHA: ${{ github.sha }}

    steps:
      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd  # v6.0.2

      - name: Set up virtual environment
        run: |
          python -m venv .venv
          echo "$PWD/.venv/bin" >> "$GITHUB_PATH"
          .venv/bin/python --version
          .venv/bin/pip --version

      - name: Cache pip packages
        uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306  # v5.0.3
        with:
          path: ~/.cache/pip
          key: pip-benchmark-${{ runner.os }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: pip-benchmark-${{ runner.os }}-

      - name: Cache Cargo registry
        uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306  # v5.0.3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
          key: cargo-benchmark-${{ runner.os }}-${{ hashFiles('rust/Cargo.lock') }}
          restore-keys: cargo-benchmark-${{ runner.os }}-

      - name: Cache Rust target
        uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306  # v5.0.3
        with:
          path: rust/target
          key: cargo-target-benchmark-${{ runner.os }}-${{ hashFiles('rust/Cargo.lock', 'rust/src/**/*.rs') }}
          restore-keys: cargo-target-benchmark-${{ runner.os }}-

      - name: Install dependencies
        run: pip install maturin pytest pytest-benchmark memory-profiler matplotlib seaborn --quiet

      - name: Build Rust extension
        run: cd rust && maturin develop --release

      - name: Verify Rust extension
        run: python -c "import wasp2_rust; print(f'wasp2_rust loaded from {wasp2_rust.__file__}')"

      - name: Install package
        run: pip install -e ".[dev]" --no-build-isolation --prefer-binary --quiet || pip install -e "." --prefer-binary --quiet

      - name: Check tool availability
        run: |
          echo "Tool availability:"
          python benchmarking/run_benchmarks.py --check-tools

      - name: Run quick benchmarks
        if: env.BENCHMARK_TYPE == 'quick'
        run: |
          python benchmarking/run_benchmarks.py --quick \
            --output "benchmarking/results/benchmark_quick_${COMMIT_SHA}.json"

      - name: Run full benchmarks
        if: env.BENCHMARK_TYPE == 'full'
        run: |
          python benchmarking/run_benchmarks.py --all \
            --n-variants 100000 \
            --n-reads 100000 \
            --iterations 10 \
            --output "benchmarking/results/benchmark_full_${COMMIT_SHA}.json"

      - name: Run counting benchmarks
        if: env.BENCHMARK_TYPE == 'counting'
        run: |
          python benchmarking/run_benchmarks.py --counting \
            --n-variants 50000 \
            --iterations 10 \
            --output "benchmarking/results/benchmark_counting_${COMMIT_SHA}.json"

      - name: Run mapping benchmarks
        if: env.BENCHMARK_TYPE == 'mapping'
        run: |
          python benchmarking/run_benchmarks.py --mapping \
            --n-reads 50000 \
            --iterations 10 \
            --output "benchmarking/results/benchmark_mapping_${COMMIT_SHA}.json"

      - name: Run concordance validation
        if: env.BENCHMARK_TYPE == 'concordance'
        run: |
          python benchmarking/run_benchmarks.py --concordance \
            --n-variants 10000 \
            --output "benchmarking/results/benchmark_concordance_${COMMIT_SHA}.json"

      - name: Run pytest benchmarks
        run: |
          python tests/benchmarks/run_benchmarks.py --quick --no-figures \
            --output-dir .benchmarks

      - name: Upload benchmark results
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f  # v6
        with:
          name: benchmark-results-${{ github.sha }}
          path: |
            benchmarking/results/*.json
            .benchmarks/*.json
          retention-days: 90

      - name: Generate summary
        run: |
          echo "## Benchmark Results" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "**Commit:** ${COMMIT_SHA}" >> "$GITHUB_STEP_SUMMARY"
          echo "**Type:** ${BENCHMARK_TYPE}" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"

          # Parse and display results if available
          if ls benchmarking/results/benchmark_*.json 1> /dev/null 2>&1; then
            echo "### Performance Metrics" >> "$GITHUB_STEP_SUMMARY"
            echo "" >> "$GITHUB_STEP_SUMMARY"
            echo "Results saved to artifacts." >> "$GITHUB_STEP_SUMMARY"
          fi

  compare-to-baseline:
    name: Compare to Baseline
    runs-on: [self-hosted, macOS, ARM64, docker, wasp2]
    timeout-minutes: 10
    permissions:
      contents: read
    needs: benchmark
    if: github.event_name == 'release' || github.event.inputs.benchmark_type == 'full'
    defaults:
      run:
        shell: bash

    env:
      COMMIT_SHA: ${{ github.sha }}

    steps:
      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd  # v6.0.2

      - name: Set up virtual environment
        run: |
          python -m venv .venv
          echo "$PWD/.venv/bin" >> "$GITHUB_PATH"

      - name: Download current results
        uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093  # v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: current_results/

      - name: Validate performance claims
        run: |
          python - <<'PYEOF'
          import json, glob, sys

          CLAIMS = {
              "filtering_speedup": {"min": 61.0, "label": "61x faster WASP filtering (vs WASP v1)"},
              "counting_speedup": {"min": 6.4, "label": "6.4x faster counting (vs phASER)"},
              "concordance_r2": {"min": 0.99, "label": "r² > 0.99 concordance with GATK"},
          }

          results_files = glob.glob("current_results/**/*.json", recursive=True)
          if not results_files:
              print("❌ No benchmark result files found")
              print("The benchmark job may have failed to produce output.")
              sys.exit(1)

          validated = {}
          errors = []
          for fpath in results_files:
              try:
                  with open(fpath) as f:
                      data = json.load(f)
              except (json.JSONDecodeError, OSError) as e:
                  errors.append(f"⚠️  Skipping {fpath}: {e}")
                  continue

              for key in CLAIMS:
                  for source in [data] + [data.get(s, {}) for s in ("summary", "metrics", "results")]:
                      if isinstance(source, dict) and key in source:
                          val = source[key]
                          if isinstance(val, (int, float)):
                              validated[key] = float(val)

          lines = ["## Performance Claims Validation", ""]
          if errors:
              for err in errors:
                  lines.append(err)
              lines.append("")

          passed = 0
          for key, claim in CLAIMS.items():
              if key in validated:
                  value = validated[key]
                  ok = value >= claim["min"]
                  status = "✅" if ok else "❌"
                  lines.append(f"- {status} {claim['label']}: **{value:.2f}** (threshold: {claim['min']})")
                  if ok:
                      passed += 1
              else:
                  lines.append(f"- ⚠️  {claim['label']}: not measured in this run")

          lines.append("")
          lines.append(f"**Result: {passed}/{len(CLAIMS)} claims validated**")

          report = "\n".join(lines)
          print(report)
          with open("validation_report.md", "w") as f:
              f.write(report)
          PYEOF

      - name: Generate comparison report
        run: |
          if [ -f validation_report.md ]; then
            cat validation_report.md >> "$GITHUB_STEP_SUMMARY"
          else
            echo "## Performance Comparison" >> "$GITHUB_STEP_SUMMARY"
            echo "" >> "$GITHUB_STEP_SUMMARY"
            echo "No benchmark results available for validation." >> "$GITHUB_STEP_SUMMARY"
          fi
