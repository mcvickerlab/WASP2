{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Performance Optimization Tutorial\n",
    "\n",
    "This tutorial provides a deep dive into optimizing WASP2 performance for large-scale analyses.\n",
    "\n",
    "**Topics covered:**\n",
    "- VCF vs BCF vs PGEN format comparison\n",
    "- Rust vs Python performance benchmarks\n",
    "- HPC/cluster deployment patterns\n",
    "- Memory optimization strategies\n",
    "\n",
    "**Prerequisites:**\n",
    "- WASP2 installed with Rust extension (`maturin develop --release -m rust/Cargo.toml`)\n",
    "- Basic familiarity with BAM/VCF formats\n",
    "- For HPC sections: Access to SLURM cluster (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toc",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Variant File Formats](#1-variant-file-formats)\n",
    "2. [Rust Acceleration](#2-rust-acceleration)\n",
    "3. [HPC Deployment](#3-hpc-deployment)\n",
    "4. [Memory Optimization](#4-memory-optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Find repository root\n",
    "repo_root = Path(\".\").resolve().parent\n",
    "if not (repo_root / \"rust\").exists():\n",
    "    repo_root = Path(\".\")\n",
    "\n",
    "# Test data paths\n",
    "test_data = repo_root / \"pipelines\" / \"nf-modules\" / \"tests\" / \"data\"\n",
    "vcf_file = test_data / \"sample.vcf.gz\"\n",
    "bam_file = test_data / \"minimal.bam\"\n",
    "\n",
    "# Check Rust extension availability\n",
    "try:\n",
    "    import wasp2_rust\n",
    "    RUST_AVAILABLE = True\n",
    "    print(\"Rust extension loaded successfully\")\n",
    "except ImportError:\n",
    "    RUST_AVAILABLE = False\n",
    "    print(\"Rust extension not available. Build with: maturin develop --release -m rust/Cargo.toml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formats-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Variant File Formats\n",
    "\n",
    "Understanding the performance characteristics of different variant file formats is crucial for optimizing large-scale genomic analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "format-comparison",
   "metadata": {},
   "source": [
    "### Format Overview\n",
    "\n",
    "| Format | Type | Read Speed | Write Speed | File Size | Use Case |\n",
    "|--------|------|------------|-------------|-----------|----------|\n",
    "| **VCF** | Text | Slowest | Slow | Largest | Human-readable, debugging |\n",
    "| **VCF.gz** | Compressed text | Slow | Slow | Medium | Standard distribution |\n",
    "| **BCF** | Binary | 5-10x faster | 3-5x faster | Smaller | Production pipelines |\n",
    "| **PGEN** | Binary (PLINK2) | 10-100x faster | Fast | Smallest | GWAS, population genetics |\n",
    "\n",
    "**Key insights:**\n",
    "- VCF is great for inspection but slow for processing\n",
    "- BCF is the binary equivalent of VCF with full compatibility\n",
    "- PGEN is optimized for genotype-only access (no INFO fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "format-demo-header",
   "metadata": {},
   "source": [
    "### Format Conversion Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert VCF to BCF (binary format for faster processing)\n",
    "# This is a common first step in production pipelines\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    bcf_out = Path(tmpdir) / \"variants.bcf\"\n",
    "    \n",
    "    # VCF -> BCF conversion\n",
    "    cmd = f\"bcftools view -Ob -o {bcf_out} {vcf_file}\"\n",
    "    result = subprocess.run(cmd.split(), capture_output=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        vcf_size = os.path.getsize(vcf_file)\n",
    "        bcf_size = os.path.getsize(bcf_out)\n",
    "        print(f\"VCF.gz size: {vcf_size:,} bytes\")\n",
    "        print(f\"BCF size: {bcf_size:,} bytes\")\n",
    "        print(f\"Compression ratio: {vcf_size/bcf_size:.2f}x\")\n",
    "    else:\n",
    "        print(\"bcftools not available - install via: conda install -c bioconda bcftools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wasp2-vcf-header",
   "metadata": {},
   "source": "### WASP2 VCF Processing\n\nWASP2's Rust extension includes a high-performance VCF parser using the `noodles` library, which is 5-6x faster than calling bcftools as a subprocess. The Rust parser supports VCF and VCF.gz formats; for BCF files, the system automatically falls back to bcftools."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wasp2-vcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUST_AVAILABLE:\n",
    "    import tempfile\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        bed_out = Path(tmpdir) / \"variants.bed\"\n",
    "        \n",
    "        # Use WASP2's Rust-powered VCF-to-BED conversion\n",
    "        start = time.perf_counter()\n",
    "        n_variants = wasp2_rust.vcf_to_bed(\n",
    "            str(vcf_file),\n",
    "            str(bed_out),\n",
    "            samples=[\"sample1\"],  # Filter to one sample\n",
    "            het_only=True,         # Only heterozygous sites\n",
    "            include_indels=False   # SNPs only\n",
    "        )\n",
    "        elapsed = time.perf_counter() - start\n",
    "        \n",
    "        print(f\"Extracted {n_variants} het variants in {elapsed*1000:.2f}ms\")\n",
    "        print(f\"\\nBED output preview:\")\n",
    "        print(bed_out.read_text()[:500])\n",
    "else:\n",
    "    print(\"Rust extension required for this example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "format-recommendations",
   "metadata": {},
   "source": [
    "### Format Recommendations\n",
    "\n",
    "| Scenario | Recommended Format | Reason |\n",
    "|----------|-------------------|--------|\n",
    "| Development/debugging | VCF | Human-readable |\n",
    "| Production WASP2 pipeline | BCF or VCF.gz | Full variant info, WASP2 compatible |\n",
    "| GWAS with millions of samples | PGEN | Optimized for genotype matrix operations |\n",
    "| Sharing/archival | VCF.gz + tabix index | Universally supported |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rust-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Rust Acceleration\n",
    "\n",
    "WASP2 uses Rust for performance-critical operations, achieving 5-61x speedups over pure Python implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rust-overview",
   "metadata": {},
   "source": "### Rust-Accelerated Functions\n\n| Function | Speedup | Description |\n|----------|---------|-------------|\n| `unified_make_reads_parallel` | **3-8x** | Single-pass BAM processing with parallel chromosome processing |\n| `intersect_bam_bed` | **41x** | BAM-BED intersection using coitrees |\n| `filter_bam_wasp` | **5x** | WASP mapping filter |\n| `vcf_to_bed` | **5-6x** | VCF to BED conversion |\n| Counting workflow | **6.4x** | Full analysis pipeline vs phASER |\n\n**Overall WASP2 mapping workflow achieves 61x speedup vs WASP v1** through combined optimizations.\n\n**Why Rust?**\n- Zero-cost abstractions\n- No garbage collection pauses\n- Safe parallelism with rayon\n- Excellent bioinformatics libraries (rust-htslib, noodles, coitrees)"
  },
  {
   "cell_type": "markdown",
   "id": "rust-benchmark-header",
   "metadata": {},
   "source": [
    "### Benchmark: Rust vs Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rust-benchmark",
   "metadata": {},
   "outputs": [],
   "source": "if RUST_AVAILABLE:\n    import tempfile\n    \n    with tempfile.TemporaryDirectory() as tmpdir:\n        bed_file = Path(tmpdir) / \"variants.bed\"\n        out_file = Path(tmpdir) / \"intersect.bed\"\n        \n        # Create BED from VCF\n        wasp2_rust.vcf_to_bed(str(vcf_file), str(bed_file))\n        \n        # Benchmark Rust intersection\n        n_iterations = 5\n        rust_times = []\n        for _ in range(n_iterations):\n            start = time.perf_counter()\n            n_intersections = wasp2_rust.intersect_bam_bed(\n                str(bam_file),\n                str(bed_file),\n                str(out_file)\n            )\n            rust_times.append(time.perf_counter() - start)\n        \n        rust_mean = sum(rust_times) / len(rust_times)\n        \n        print(f\"Rust intersect_bam_bed: {rust_mean*1000:.3f}ms (mean of {n_iterations} runs)\")\n        print(f\"Found {n_intersections} read-variant overlaps\")\n        print(f\"\\nExpected speedup vs pybedtools: ~41x\")\n        print(f\"Expected speedup vs samtools pipeline: ~4-5x\")\nelse:\n    print(\"Rust extension required for benchmarks\")"
  },
  {
   "cell_type": "markdown",
   "id": "rust-usage-header",
   "metadata": {},
   "source": [
    "### Using Rust Functions Directly\n",
    "\n",
    "You can access Rust-accelerated functions directly via the `wasp2_rust` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rust-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUST_AVAILABLE:\n",
    "    # List available Rust functions\n",
    "    rust_functions = [name for name in dir(wasp2_rust) if not name.startswith('_')]\n",
    "    print(\"Available Rust functions:\")\n",
    "    for func in rust_functions:\n",
    "        doc = getattr(wasp2_rust, func).__doc__\n",
    "        if doc:\n",
    "            first_line = doc.strip().split('\\n')[0]\n",
    "            print(f\"  {func}: {first_line[:60]}...\")\n",
    "        else:\n",
    "            print(f\"  {func}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rust-parallel",
   "metadata": {},
   "source": [
    "### Parallel Processing Configuration\n",
    "\n",
    "The unified pipeline supports parallel processing across chromosomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration options for unified_make_reads_parallel\n",
    "config_options = {\n",
    "    \"threads\": 8,           # Number of worker threads (0 = auto-detect)\n",
    "    \"max_seqs\": 64,         # Max haplotype sequences per read pair\n",
    "    \"channel_buffer\": 50000, # Channel buffer for streaming\n",
    "    \"compression_threads\": 4, # Threads for gzip compression\n",
    "    \"compress_output\": True,  # Output .fq.gz instead of .fq\n",
    "}\n",
    "\n",
    "print(\"Recommended parallel configuration:\")\n",
    "for key, value in config_options.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nThread scaling guidelines:\")\n",
    "print(\"  - 4 threads: Good for laptops, ~3x speedup\")\n",
    "print(\"  - 8 threads: Workstation default, ~5x speedup\")\n",
    "print(\"  - 16+ threads: HPC nodes, ~8x speedup (diminishing returns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hpc-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. HPC Deployment\n",
    "\n",
    "WASP2 is designed for high-performance computing environments. This section covers deployment patterns for SLURM clusters and integration with workflow managers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "slurm-header",
   "metadata": {},
   "source": [
    "### SLURM Job Submission\n",
    "\n",
    "Example SLURM job script for running WASP2 on a cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "slurm-script",
   "metadata": {},
   "outputs": [],
   "source": "slurm_template = '''#!/bin/bash\n#SBATCH --job-name=wasp2_analysis\n#SBATCH --partition=normal\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=16\n#SBATCH --mem=64G\n#SBATCH --time=4:00:00\n#SBATCH --output=wasp2_%j.log\n\n# Load required modules (adjust for your cluster)\nmodule load anaconda3\nmodule load samtools/1.17\n\n# Activate WASP2 environment\nconda activate WASP2\n\n# Run WASP2 pipeline with explicit thread count\nwasp2-map make-reads \\\\\n    --bam input.bam \\\\\n    --vcf variants.vcf.gz \\\\\n    --sample NA12878 \\\\\n    --threads ${SLURM_CPUS_PER_TASK} \\\\\n    --out_dir results/\n\necho \"WASP2 completed successfully\"\n'''\n\nprint(\"Example SLURM job script:\")\nprint(slurm_template)"
  },
  {
   "cell_type": "markdown",
   "id": "nextflow-header",
   "metadata": {},
   "source": [
    "### Nextflow Integration\n",
    "\n",
    "WASP2 includes Nextflow modules in `pipelines/nf-modules/` for workflow orchestration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nextflow-list",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available Nextflow modules\n",
    "nf_modules = repo_root / \"pipelines\" / \"nf-modules\"\n",
    "if nf_modules.exists():\n",
    "    print(\"Available Nextflow modules:\")\n",
    "    for module in sorted(nf_modules.glob(\"**/*.nf\")):\n",
    "        rel_path = module.relative_to(nf_modules)\n",
    "        print(f\"  {rel_path}\")\n",
    "else:\n",
    "    print(\"Nextflow modules directory not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nextflow-example",
   "metadata": {},
   "outputs": [],
   "source": "# Example Nextflow workflow (conceptual - adjust module paths for your setup)\nnextflow_example = '''#!/usr/bin/env nextflow\n\n// WASP2 RNA-seq allelic imbalance pipeline\nnextflow.enable.dsl = 2\n\n// Include WASP2 modules (adjust paths to match your installation)\n// Actual modules are in pipelines/nf-modules/modules/wasp2/\ninclude { MAP } from './modules/wasp2/map/main'\ninclude { COUNT } from './modules/wasp2/count/main'\ninclude { ANALYZE } from './modules/wasp2/analyze/main'\n\nworkflow {\n    // Input channels\n    bam_ch = Channel.fromPath(params.bams)\n    vcf_ch = Channel.value(file(params.vcf))\n    \n    // Run WASP mapping filter (removes mapping bias)\n    MAP(bam_ch, vcf_ch)\n    \n    // Count alleles at heterozygous sites\n    COUNT(MAP.out.filtered_bam, vcf_ch)\n    \n    // Analyze allelic imbalance\n    ANALYZE(COUNT.out.counts)\n}\n'''\n\nprint(\"Example Nextflow workflow:\")\nprint(nextflow_example)"
  },
  {
   "cell_type": "markdown",
   "id": "container-header",
   "metadata": {},
   "source": [
    "### Container Deployment (Singularity/Apptainer)\n",
    "\n",
    "For HPC clusters that don't allow Docker, use Singularity/Apptainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "singularity-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "singularity_usage = '''# Pull the WASP2 container\n",
    "singularity pull wasp2.sif docker://ghcr.io/your-org/wasp2:latest\n",
    "\n",
    "# Run WASP2 via Singularity\n",
    "singularity exec --bind /data:/data wasp2.sif \\\n",
    "    wasp2-count count-variants \\\n",
    "    /data/input.bam \\\n",
    "    /data/variants.vcf.gz \\\n",
    "    --out_file /data/counts.tsv\n",
    "\n",
    "# With GPU support (for future ML features)\n",
    "singularity exec --nv --bind /data:/data wasp2.sif \\\n",
    "    wasp2-analyze find-imbalance /data/counts.tsv\n",
    "'''\n",
    "\n",
    "print(\"Singularity/Apptainer usage:\")\n",
    "print(singularity_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Memory Optimization\n",
    "\n",
    "Processing large BAM files requires careful memory management. This section covers strategies for reducing memory footprint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-strategies",
   "metadata": {},
   "source": [
    "### Memory Usage Patterns\n",
    "\n",
    "| Component | Memory Scaling | Optimization Strategy |\n",
    "|-----------|---------------|----------------------|\n",
    "| BAM reading | O(buffer_size) | Use streaming, avoid loading full file |\n",
    "| Variant lookup | O(n_variants) | Use interval trees (coitrees) |\n",
    "| Read pairs | O(pairs_in_flight) | Tune `pair_buffer_reserve` |\n",
    "| Haplotypes | O(max_seqs) | Limit with `max_seqs` parameter |\n",
    "| Output | O(channel_buffer) | Stream to disk, avoid buffering |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-header",
   "metadata": {},
   "source": [
    "### Streaming vs Loading\n",
    "\n",
    "WASP2's Rust implementation uses streaming patterns to minimize memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_diagram = '''\n",
    "BAM File (100GB)          Variant Tree (100MB)          FASTQ Output\n",
    "     |                          |                            |\n",
    "     v                          v                            v\n",
    "+---------+              +-------------+              +-----------+\n",
    "| Stream  |  ------>     | coitrees    |  ------>     | Write     |\n",
    "| Reader  |  (1 pair     | O(log n)    |  (stream     | Channel   |\n",
    "| (low    |   at time)   | lookup      |   results)   | (50K buf) |\n",
    "| memory) |              |             |              |           |\n",
    "+---------+              +-------------+              +-----------+\n",
    "\n",
    "Peak memory: ~500MB - 2GB (independent of BAM size!)\n",
    "'''\n",
    "\n",
    "print(\"WASP2 streaming architecture:\")\n",
    "print(streaming_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-tuning",
   "metadata": {},
   "source": [
    "### Memory Tuning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-params",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_configs = {\n",
    "    \"Low memory (laptop, 8GB RAM)\": {\n",
    "        \"pair_buffer_reserve\": 50000,\n",
    "        \"channel_buffer\": 10000,\n",
    "        \"max_seqs\": 32,\n",
    "        \"threads\": 4,\n",
    "        \"estimated_peak\": \"~500MB\",\n",
    "    },\n",
    "    \"Standard (workstation, 32GB RAM)\": {\n",
    "        \"pair_buffer_reserve\": 100000,\n",
    "        \"channel_buffer\": 50000,\n",
    "        \"max_seqs\": 64,\n",
    "        \"threads\": 8,\n",
    "        \"estimated_peak\": \"~2GB\",\n",
    "    },\n",
    "    \"High memory (HPC node, 128GB+ RAM)\": {\n",
    "        \"pair_buffer_reserve\": 500000,\n",
    "        \"channel_buffer\": 100000,\n",
    "        \"max_seqs\": 128,\n",
    "        \"threads\": 16,\n",
    "        \"estimated_peak\": \"~8GB\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Memory configuration profiles:\\n\")\n",
    "for profile, config in memory_configs.items():\n",
    "    print(f\"{profile}:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunked-header",
   "metadata": {},
   "source": [
    "### Chunked Processing for Very Large Datasets\n",
    "\n",
    "For datasets too large to process in one pass, split by chromosome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunked-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_script = '''#!/bin/bash\n",
    "# Process BAM chromosome-by-chromosome to reduce memory\n",
    "\n",
    "BAM=$1\n",
    "VCF=$2\n",
    "OUTDIR=$3\n",
    "\n",
    "# Get chromosome list from BAM\n",
    "CHROMS=$(samtools view -H $BAM | grep \"^@SQ\" | cut -f2 | sed 's/SN://')\n",
    "\n",
    "# Process each chromosome separately\n",
    "for CHR in $CHROMS; do\n",
    "    echo \"Processing $CHR...\"\n",
    "    \n",
    "    # Extract chromosome\n",
    "    samtools view -b $BAM $CHR > ${OUTDIR}/${CHR}.bam\n",
    "    samtools index ${OUTDIR}/${CHR}.bam\n",
    "    \n",
    "    # Run WASP2 on subset\n",
    "    wasp2-map make-reads \\\n",
    "        --bam ${OUTDIR}/${CHR}.bam \\\n",
    "        --vcf $VCF \\\n",
    "        --region $CHR \\\n",
    "        --out_dir ${OUTDIR}/${CHR}/\n",
    "    \n",
    "    # Clean up intermediate file\n",
    "    rm ${OUTDIR}/${CHR}.bam*\n",
    "done\n",
    "\n",
    "# Merge results\n",
    "cat ${OUTDIR}/*/counts.tsv > ${OUTDIR}/all_counts.tsv\n",
    "'''\n",
    "\n",
    "print(\"Chunked processing script:\")\n",
    "print(chunked_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "profiling-header",
   "metadata": {},
   "source": [
    "### Memory Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "profiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiling_example = '''# Profile memory usage with memory_profiler\n",
    "pip install memory_profiler\n",
    "\n",
    "# Run with memory profiling\n",
    "mprof run wasp2-map make-reads --bam input.bam --vcf variants.vcf.gz\n",
    "\n",
    "# View memory plot\n",
    "mprof plot\n",
    "\n",
    "# Or use peak memory reporting\n",
    "/usr/bin/time -v wasp2-map make-reads --bam input.bam --vcf variants.vcf.gz 2>&1 | \\\n",
    "    grep \"Maximum resident set size\"\n",
    "'''\n",
    "\n",
    "print(\"Memory profiling commands:\")\n",
    "print(profiling_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "1. **Format choice matters**: Use BCF for production, VCF for debugging\n",
    "2. **Leverage Rust acceleration**: 5-61x speedups available via `wasp2_rust` module\n",
    "3. **Scale to HPC**: Use SLURM scripts or Nextflow for cluster deployment\n",
    "4. **Tune memory**: Adjust buffer sizes based on available RAM\n",
    "\n",
    "**Further reading:**\n",
    "- [WASP2 Benchmarking Framework](../benchmarking/README.md)\n",
    "- [Nextflow Modules Documentation](../pipelines/nf-modules/README.md)\n",
    "- [Rust Extension Source](../rust/src/lib.rs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}