{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Performance Optimization Tutorial\n",
    "\n",
    "This tutorial provides a deep dive into optimizing WASP2 performance for large-scale analyses.\n",
    "\n",
    "**Topics covered:**\n",
    "- VCF vs BCF vs PGEN format comparison\n",
    "- Rust vs Python performance benchmarks\n",
    "- HPC/cluster deployment patterns\n",
    "- Memory optimization strategies\n",
    "\n",
    "**Prerequisites:**\n",
    "- WASP2 installed with Rust extension (`maturin develop --release -m rust/Cargo.toml`)\n",
    "- Basic familiarity with BAM/VCF formats\n",
    "- For HPC sections: Access to SLURM cluster (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toc",
   "metadata": {},
   "source": "## Table of Contents\n\n1. [Variant File Formats](#1-variant-file-formats)\n2. [Rust Acceleration](#2-rust-acceleration)\n3. [HPC Deployment](#3-hpc-deployment)\n4. [Memory Optimization](#4-memory-optimization)\n5. [Input Validation & Troubleshooting](#5-input-validation--troubleshooting)"
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": "import time\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef validate_file(path: Path, description: str) -> bool:\n    \"\"\"Validate file exists and is readable.\"\"\"\n    if not path.exists():\n        print(f\"WARNING: {description} not found: {path}\")\n        return False\n    if not path.is_file():\n        print(f\"WARNING: {description} is not a file: {path}\")\n        return False\n    return True\n\ndef check_command(cmd: str) -> bool:\n    \"\"\"Check if a command is available in PATH.\"\"\"\n    try:\n        result = subprocess.run(\n            [\"which\", cmd], capture_output=True, text=True, timeout=5\n        )\n        return result.returncode == 0\n    except subprocess.TimeoutExpired:\n        print(f\"WARNING: Timeout checking for {cmd}\")\n        return False\n    except OSError as e:\n        print(f\"WARNING: System error checking for {cmd}: {e}\")\n        return False\n    except Exception as e:\n        print(f\"WARNING: Unexpected error checking for {cmd}: {type(e).__name__}: {e}\")\n        return False\n\n# Find repository root with validation\nrepo_root = Path(\".\").resolve().parent\nif not (repo_root / \"rust\").exists():\n    repo_root = Path(\".\")\n    if not (repo_root / \"rust\").exists():\n        print(\"WARNING: Could not locate WASP2 repository root\")\n\n# Test data paths with validation\ntest_data = repo_root / \"pipelines\" / \"nf-modules\" / \"tests\" / \"data\"\nvcf_file = test_data / \"sample.vcf.gz\"\nbam_file = test_data / \"minimal.bam\"\n\n# Validate test files\nfiles_valid = all([\n    validate_file(vcf_file, \"VCF file\"),\n    validate_file(bam_file, \"BAM file\"),\n])\n\n# Check external tool availability\nBCFTOOLS_AVAILABLE = check_command(\"bcftools\")\nSAMTOOLS_AVAILABLE = check_command(\"samtools\")\n\nif not BCFTOOLS_AVAILABLE:\n    print(\"INFO: bcftools not found - some examples will be skipped\")\nif not SAMTOOLS_AVAILABLE:\n    print(\"INFO: samtools not found - some examples will be skipped\")\n\n# Check Rust extension availability\ntry:\n    import wasp2_rust\n    RUST_AVAILABLE = True\n    print(\"Rust extension loaded successfully\")\nexcept ImportError as e:\n    RUST_AVAILABLE = False\n    print(f\"Rust extension not available: {e}\")\n    print(\"Build with: maturin develop --release -m rust/Cargo.toml\")\n\nprint(f\"\\nEnvironment summary:\")\nprint(f\"  Python: {sys.version.split()[0]}\")\nprint(f\"  Rust extension: {'available' if RUST_AVAILABLE else 'not available'}\")\nprint(f\"  bcftools: {'available' if BCFTOOLS_AVAILABLE else 'not available'}\")\nprint(f\"  samtools: {'available' if SAMTOOLS_AVAILABLE else 'not available'}\")\nprint(f\"  Test data: {'valid' if files_valid else 'missing'}\")"
  },
  {
   "cell_type": "markdown",
   "id": "formats-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Variant File Formats\n",
    "\n",
    "Understanding the performance characteristics of different variant file formats is crucial for optimizing large-scale genomic analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "format-comparison",
   "metadata": {},
   "source": [
    "### Format Overview\n",
    "\n",
    "| Format | Type | Read Speed | Write Speed | File Size | Use Case |\n",
    "|--------|------|------------|-------------|-----------|----------|\n",
    "| **VCF** | Text | Slowest | Slow | Largest | Human-readable, debugging |\n",
    "| **VCF.gz** | Compressed text | Slow | Slow | Medium | Standard distribution |\n",
    "| **BCF** | Binary | 5-10x faster | 3-5x faster | Smaller | Production pipelines |\n",
    "| **PGEN** | Binary (PLINK2) | 10-100x faster | Fast | Smallest | GWAS, population genetics |\n",
    "\n",
    "**Key insights:**\n",
    "- VCF is great for inspection but slow for processing\n",
    "- BCF is the binary equivalent of VCF with full compatibility\n",
    "- PGEN is optimized for genotype-only access (no INFO fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "format-demo-header",
   "metadata": {},
   "source": [
    "### Format Conversion Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format-conversion",
   "metadata": {},
   "outputs": [],
   "source": "# Convert VCF to BCF (binary format for faster processing)\n# This is a common first step in production pipelines\n\nimport tempfile\nimport os\n\nif not BCFTOOLS_AVAILABLE:\n    print(\"Skipping: bcftools not available\")\n    print(\"Install via: conda install -c bioconda bcftools\")\nelif not validate_file(vcf_file, \"Input VCF\"):\n    print(\"Skipping: Input VCF file not found\")\nelse:\n    try:\n        # Validate input VCF has content\n        vcf_size = os.path.getsize(vcf_file)\n        if vcf_size == 0:\n            print(\"WARNING: Input VCF file is empty (0 bytes)\")\n        else:\n            with tempfile.TemporaryDirectory() as tmpdir:\n                bcf_out = Path(tmpdir) / \"variants.bcf\"\n                \n                # VCF -> BCF conversion with error capture\n                cmd = [\"bcftools\", \"view\", \"-Ob\", \"-o\", str(bcf_out), str(vcf_file)]\n                result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)\n                \n                if result.returncode == 0:\n                    bcf_size = os.path.getsize(bcf_out)\n                    print(f\"VCF.gz size: {vcf_size:,} bytes\")\n                    print(f\"BCF size: {bcf_size:,} bytes\")\n                    \n                    if bcf_size > 0:\n                        print(f\"Compression ratio: {vcf_size/bcf_size:.2f}x\")\n                    else:\n                        print(\"WARNING: BCF file is empty\")\n                else:\n                    print(f\"bcftools failed with exit code {result.returncode}\")\n                    if result.stderr:\n                        print(f\"Error: {result.stderr[:200]}\")\n    except subprocess.TimeoutExpired:\n        print(\"ERROR: bcftools timed out after 60 seconds\")\n    except Exception as e:\n        print(f\"ERROR: Unexpected error during conversion: {type(e).__name__}: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "wasp2-vcf-header",
   "metadata": {},
   "source": "### WASP2 VCF Processing\n\nWASP2's Rust extension includes a high-performance VCF parser using the `noodles` library, which is 5-6x faster than calling bcftools as a subprocess. The Rust parser supports VCF and VCF.gz formats; for BCF files, the system automatically falls back to bcftools."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wasp2-vcf",
   "metadata": {},
   "outputs": [],
   "source": "if not RUST_AVAILABLE:\n    print(\"Skipping: Rust extension required for this example\")\nelif not validate_file(vcf_file, \"Input VCF\"):\n    print(\"Skipping: Input VCF file not found\")\nelse:\n    import tempfile\n    \n    try:\n        with tempfile.TemporaryDirectory() as tmpdir:\n            bed_out = Path(tmpdir) / \"variants.bed\"\n            \n            # Use WASP2's Rust-powered VCF-to-BED conversion\n            start = time.perf_counter()\n            n_variants = wasp2_rust.vcf_to_bed(\n                str(vcf_file),\n                str(bed_out),\n                samples=[\"sample1\"],  # Filter to one sample\n                het_only=True,         # Only heterozygous sites\n                include_indels=False   # SNPs only\n            )\n            elapsed = time.perf_counter() - start\n            \n            print(f\"Extracted {n_variants} het variants in {elapsed*1000:.2f}ms\")\n            \n            if n_variants > 0 and bed_out.exists():\n                content = bed_out.read_text()\n                print(f\"\\nBED output preview ({len(content)} bytes):\")\n                print(content[:500] if len(content) > 500 else content)\n            elif n_variants == 0:\n                print(\"\\nNo heterozygous variants found for sample1\")\n            else:\n                print(\"\\nWARNING: Output file not created\")\n                \n    except RuntimeError as e:\n        print(f\"Rust function error: {e}\")\n        print(\"TIP: Check that sample name exists in VCF header\")\n    except Exception as e:\n        print(f\"Unexpected error: {type(e).__name__}: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "format-recommendations",
   "metadata": {},
   "source": [
    "### Format Recommendations\n",
    "\n",
    "| Scenario | Recommended Format | Reason |\n",
    "|----------|-------------------|--------|\n",
    "| Development/debugging | VCF | Human-readable |\n",
    "| Production WASP2 pipeline | BCF or VCF.gz | Full variant info, WASP2 compatible |\n",
    "| GWAS with millions of samples | PGEN | Optimized for genotype matrix operations |\n",
    "| Sharing/archival | VCF.gz + tabix index | Universally supported |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rust-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Rust Acceleration\n",
    "\n",
    "WASP2 uses Rust for performance-critical operations, achieving 5-61x speedups over pure Python implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rust-overview",
   "metadata": {},
   "source": "### Rust-Accelerated Functions\n\n| Function | Speedup | Description |\n|----------|---------|-------------|\n| `unified_make_reads_parallel` | **3-8x** | Single-pass BAM processing with parallel chromosome processing |\n| `intersect_bam_bed` | **41x** | BAM-BED intersection using coitrees |\n| `filter_bam_wasp` | **5x** | WASP mapping filter |\n| `vcf_to_bed` | **5-6x** | VCF to BED conversion |\n| Counting workflow | **6.4x** | Full analysis pipeline vs phASER |\n\n**Overall WASP2 mapping workflow achieves 61x speedup vs WASP v1** through combined optimizations.\n\n**Why Rust?**\n- Zero-cost abstractions\n- No garbage collection pauses\n- Safe parallelism with rayon\n- Excellent bioinformatics libraries (rust-htslib, noodles, coitrees)"
  },
  {
   "cell_type": "markdown",
   "id": "rust-benchmark-header",
   "metadata": {},
   "source": [
    "### Benchmark: Rust vs Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rust-benchmark",
   "metadata": {},
   "outputs": [],
   "source": "if not RUST_AVAILABLE:\n    print(\"Skipping: Rust extension required for benchmarks\")\nelif not all([validate_file(vcf_file, \"VCF\"), validate_file(bam_file, \"BAM\")]):\n    print(\"Skipping: Required input files not found\")\nelse:\n    import tempfile\n    \n    try:\n        with tempfile.TemporaryDirectory() as tmpdir:\n            bed_file = Path(tmpdir) / \"variants.bed\"\n            out_file = Path(tmpdir) / \"intersect.bed\"\n            \n            # Create BED from VCF\n            n_variants = wasp2_rust.vcf_to_bed(str(vcf_file), str(bed_file))\n            \n            if n_variants == 0:\n                print(\"WARNING: No variants extracted from VCF\")\n                print(\"TIP: Verify VCF contains variants with: bcftools view -H <vcf> | head\")\n            elif not bed_file.exists():\n                print(\"ERROR: BED file was not created\")\n            else:\n                # Benchmark Rust intersection\n                n_iterations = 5\n                rust_times = []\n                n_intersections = 0\n                \n                for i in range(n_iterations):\n                    start = time.perf_counter()\n                    n_intersections = wasp2_rust.intersect_bam_bed(\n                        str(bam_file),\n                        str(bed_file),\n                        str(out_file)\n                    )\n                    rust_times.append(time.perf_counter() - start)\n                \n                rust_mean = sum(rust_times) / len(rust_times)\n                rust_std = (sum((t - rust_mean)**2 for t in rust_times) / len(rust_times))**0.5\n                \n                print(f\"Rust intersect_bam_bed benchmark results:\")\n                print(f\"  Mean: {rust_mean*1000:.3f}ms (+/- {rust_std*1000:.3f}ms)\")\n                print(f\"  Min:  {min(rust_times)*1000:.3f}ms\")\n                print(f\"  Max:  {max(rust_times)*1000:.3f}ms\")\n                print(f\"  Iterations: {n_iterations}\")\n                print(f\"\\nFound {n_intersections} read-variant overlaps\")\n                print(f\"\\nExpected speedup vs pybedtools: ~41x\")\n                print(f\"Expected speedup vs samtools pipeline: ~4-5x\")\n                \n    except RuntimeError as e:\n        print(f\"Rust error: {e}\")\n        print(\"TIP: Check that input files are valid and properly formatted\")\n    except Exception as e:\n        print(f\"Benchmark failed: {type(e).__name__}: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "rust-usage-header",
   "metadata": {},
   "source": [
    "### Using Rust Functions Directly\n",
    "\n",
    "You can access Rust-accelerated functions directly via the `wasp2_rust` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rust-usage",
   "metadata": {},
   "outputs": [],
   "source": "if not RUST_AVAILABLE:\n    print(\"Skipping: Rust extension required\")\nelse:\n    try:\n        rust_functions = [name for name in dir(wasp2_rust) if not name.startswith('_')]\n        print(\"Available Rust functions:\")\n        for func in rust_functions:\n            try:\n                doc = getattr(wasp2_rust, func).__doc__\n                if doc:\n                    first_line = doc.strip().split('\\n')[0]\n                    print(f\"  {func}: {first_line[:60]}...\")\n                else:\n                    print(f\"  {func}\")\n            except Exception as e:\n                print(f\"  {func}: (error reading docstring: {e})\")\n    except Exception as e:\n        print(f\"ERROR: Failed to list Rust functions: {type(e).__name__}: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "rust-parallel",
   "metadata": {},
   "source": [
    "### Parallel Processing Configuration\n",
    "\n",
    "The unified pipeline supports parallel processing across chromosomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration options for unified_make_reads_parallel\n",
    "config_options = {\n",
    "    \"threads\": 8,           # Number of worker threads (0 = auto-detect)\n",
    "    \"max_seqs\": 64,         # Max haplotype sequences per read pair\n",
    "    \"channel_buffer\": 50000, # Channel buffer for streaming\n",
    "    \"compression_threads\": 4, # Threads for gzip compression\n",
    "    \"compress_output\": True,  # Output .fq.gz instead of .fq\n",
    "}\n",
    "\n",
    "print(\"Recommended parallel configuration:\")\n",
    "for key, value in config_options.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nThread scaling guidelines:\")\n",
    "print(\"  - 4 threads: Good for laptops, ~3x speedup\")\n",
    "print(\"  - 8 threads: Workstation default, ~5x speedup\")\n",
    "print(\"  - 16+ threads: HPC nodes, ~8x speedup (diminishing returns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hpc-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. HPC Deployment\n",
    "\n",
    "WASP2 is designed for high-performance computing environments. This section covers deployment patterns for SLURM clusters and integration with workflow managers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "slurm-header",
   "metadata": {},
   "source": [
    "### SLURM Job Submission\n",
    "\n",
    "Example SLURM job script for running WASP2 on a cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "slurm-script",
   "metadata": {},
   "outputs": [],
   "source": "slurm_template = '''#!/bin/bash\n#SBATCH --job-name=wasp2_analysis\n#SBATCH --partition=normal\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=16\n#SBATCH --mem=64G\n#SBATCH --time=4:00:00\n#SBATCH --output=wasp2_%j.log\n#SBATCH --error=wasp2_%j.err\n#SBATCH --mail-type=FAIL,END\n#SBATCH --mail-user=${USER}@example.com\n\n# Strict error handling\nset -euo pipefail\ntrap 'echo \"ERROR: Script failed at line $LINENO with exit code $?\" >&2' ERR\n\n# Function for logging with timestamps\nlog() {\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $*\"\n}\n\n# Validate inputs before starting\nvalidate_inputs() {\n    local bam=\"$1\"\n    local vcf=\"$2\"\n    \n    if [[ ! -f \"$bam\" ]]; then\n        log \"ERROR: BAM file not found: $bam\"\n        exit 1\n    fi\n    \n    if [[ ! -f \"${bam}.bai\" && ! -f \"${bam%.bam}.bai\" ]]; then\n        log \"ERROR: BAM index not found. Run: samtools index $bam\"\n        exit 1\n    fi\n    \n    if [[ ! -f \"$vcf\" ]]; then\n        log \"ERROR: VCF file not found: $vcf\"\n        exit 1\n    fi\n    \n    log \"Input validation passed\"\n}\n\n# Check disk space (require at least 50GB free)\ncheck_disk_space() {\n    local dir=\"$1\"\n    local required_gb=50\n    local available_gb=$(df -BG \"$dir\" | tail -1 | awk '{print $4}' | tr -d 'G')\n    \n    if [[ \"$available_gb\" -lt \"$required_gb\" ]]; then\n        log \"ERROR: Insufficient disk space. Need ${required_gb}GB, have ${available_gb}GB\"\n        exit 1\n    fi\n    log \"Disk space check passed (${available_gb}GB available)\"\n}\n\n# Main execution\nlog \"Starting WASP2 analysis job ${SLURM_JOB_ID}\"\nlog \"Node: ${SLURM_NODELIST}, CPUs: ${SLURM_CPUS_PER_TASK}\"\n\n# Load required modules (adjust for your cluster)\nmodule load anaconda3 || { log \"ERROR: Failed to load anaconda3\"; exit 1; }\nmodule load samtools/1.17 || { log \"ERROR: Failed to load samtools\"; exit 1; }\n\n# Activate WASP2 environment\nconda activate WASP2 || { log \"ERROR: Failed to activate WASP2 environment\"; exit 1; }\n\n# Configuration\nBAM=\"input.bam\"\nVCF=\"variants.vcf.gz\"\nSAMPLE=\"NA12878\"\nOUTDIR=\"results\"\n\n# Validate inputs\nvalidate_inputs \"$BAM\" \"$VCF\"\ncheck_disk_space \"$OUTDIR\"\n\n# Create output directory\nmkdir -p \"$OUTDIR\"\n\n# Run WASP2 pipeline with explicit thread count\nlog \"Starting WASP2 pipeline...\"\nwasp2-map make-reads \\\\\n    --bam \"$BAM\" \\\\\n    --vcf \"$VCF\" \\\\\n    --sample \"$SAMPLE\" \\\\\n    --threads ${SLURM_CPUS_PER_TASK} \\\\\n    --out_dir \"$OUTDIR\"\n\n# Verify output\nif [[ -f \"${OUTDIR}/remap_r1.fq.gz\" ]]; then\n    log \"SUCCESS: WASP2 completed successfully\"\n    log \"Output files:\"\n    ls -lh \"${OUTDIR}/\"*.fq.gz 2>/dev/null || true\nelse\n    log \"WARNING: Expected output files not found\"\n    exit 1\nfi\n'''\n\nprint(\"Hardened SLURM job script with error handling:\")\nprint(slurm_template)"
  },
  {
   "cell_type": "markdown",
   "id": "nextflow-header",
   "metadata": {},
   "source": [
    "### Nextflow Integration\n",
    "\n",
    "WASP2 includes Nextflow modules in `pipelines/nf-modules/` for workflow orchestration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nextflow-list",
   "metadata": {},
   "outputs": [],
   "source": "# List available Nextflow modules\nnf_modules = repo_root / \"pipelines\" / \"nf-modules\"\nif not nf_modules.exists():\n    print(\"Nextflow modules directory not found\")\nelse:\n    try:\n        print(\"Available Nextflow modules:\")\n        modules = sorted(nf_modules.glob(\"**/*.nf\"))\n        if not modules:\n            print(\"  (no .nf files found)\")\n        for module in modules:\n            try:\n                rel_path = module.relative_to(nf_modules)\n                print(f\"  {rel_path}\")\n            except ValueError:\n                print(f\"  {module}\")\n    except PermissionError as e:\n        print(f\"ERROR: Permission denied accessing modules: {e}\")\n    except Exception as e:\n        print(f\"ERROR: Failed to list modules: {type(e).__name__}: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nextflow-example",
   "metadata": {},
   "outputs": [],
   "source": "# Example Nextflow workflow (conceptual - adjust module paths for your setup)\nnextflow_example = '''#!/usr/bin/env nextflow\n\n// WASP2 RNA-seq allelic imbalance pipeline\nnextflow.enable.dsl = 2\n\n// Include WASP2 modules (adjust paths to match your installation)\n// Actual modules are in pipelines/nf-modules/modules/wasp2/\ninclude { MAP } from './modules/wasp2/map/main'\ninclude { COUNT } from './modules/wasp2/count/main'\ninclude { ANALYZE } from './modules/wasp2/analyze/main'\n\nworkflow {\n    // Input channels\n    bam_ch = Channel.fromPath(params.bams)\n    vcf_ch = Channel.value(file(params.vcf))\n    \n    // Run WASP mapping filter (removes mapping bias)\n    MAP(bam_ch, vcf_ch)\n    \n    // Count alleles at heterozygous sites\n    COUNT(MAP.out.filtered_bam, vcf_ch)\n    \n    // Analyze allelic imbalance\n    ANALYZE(COUNT.out.counts)\n}\n'''\n\nprint(\"Example Nextflow workflow:\")\nprint(nextflow_example)"
  },
  {
   "cell_type": "markdown",
   "id": "container-header",
   "metadata": {},
   "source": [
    "### Container Deployment (Singularity/Apptainer)\n",
    "\n",
    "For HPC clusters that don't allow Docker, use Singularity/Apptainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "singularity-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "singularity_usage = '''# Pull the WASP2 container\n",
    "singularity pull wasp2.sif docker://ghcr.io/your-org/wasp2:latest\n",
    "\n",
    "# Run WASP2 via Singularity\n",
    "singularity exec --bind /data:/data wasp2.sif \\\n",
    "    wasp2-count count-variants \\\n",
    "    /data/input.bam \\\n",
    "    /data/variants.vcf.gz \\\n",
    "    --out_file /data/counts.tsv\n",
    "\n",
    "# With GPU support (for future ML features)\n",
    "singularity exec --nv --bind /data:/data wasp2.sif \\\n",
    "    wasp2-analyze find-imbalance /data/counts.tsv\n",
    "'''\n",
    "\n",
    "print(\"Singularity/Apptainer usage:\")\n",
    "print(singularity_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Memory Optimization\n",
    "\n",
    "Processing large BAM files requires careful memory management. This section covers strategies for reducing memory footprint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-strategies",
   "metadata": {},
   "source": [
    "### Memory Usage Patterns\n",
    "\n",
    "| Component | Memory Scaling | Optimization Strategy |\n",
    "|-----------|---------------|----------------------|\n",
    "| BAM reading | O(buffer_size) | Use streaming, avoid loading full file |\n",
    "| Variant lookup | O(n_variants) | Use interval trees (coitrees) |\n",
    "| Read pairs | O(pairs_in_flight) | Tune `pair_buffer_reserve` |\n",
    "| Haplotypes | O(max_seqs) | Limit with `max_seqs` parameter |\n",
    "| Output | O(channel_buffer) | Stream to disk, avoid buffering |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-header",
   "metadata": {},
   "source": [
    "### Streaming vs Loading\n",
    "\n",
    "WASP2's Rust implementation uses streaming patterns to minimize memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_diagram = '''\n",
    "BAM File (100GB)          Variant Tree (100MB)          FASTQ Output\n",
    "     |                          |                            |\n",
    "     v                          v                            v\n",
    "+---------+              +-------------+              +-----------+\n",
    "| Stream  |  ------>     | coitrees    |  ------>     | Write     |\n",
    "| Reader  |  (1 pair     | O(log n)    |  (stream     | Channel   |\n",
    "| (low    |   at time)   | lookup      |   results)   | (50K buf) |\n",
    "| memory) |              |             |              |           |\n",
    "+---------+              +-------------+              +-----------+\n",
    "\n",
    "Peak memory: ~500MB - 2GB (independent of BAM size!)\n",
    "'''\n",
    "\n",
    "print(\"WASP2 streaming architecture:\")\n",
    "print(streaming_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-tuning",
   "metadata": {},
   "source": [
    "### Memory Tuning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-params",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_configs = {\n",
    "    \"Low memory (laptop, 8GB RAM)\": {\n",
    "        \"pair_buffer_reserve\": 50000,\n",
    "        \"channel_buffer\": 10000,\n",
    "        \"max_seqs\": 32,\n",
    "        \"threads\": 4,\n",
    "        \"estimated_peak\": \"~500MB\",\n",
    "    },\n",
    "    \"Standard (workstation, 32GB RAM)\": {\n",
    "        \"pair_buffer_reserve\": 100000,\n",
    "        \"channel_buffer\": 50000,\n",
    "        \"max_seqs\": 64,\n",
    "        \"threads\": 8,\n",
    "        \"estimated_peak\": \"~2GB\",\n",
    "    },\n",
    "    \"High memory (HPC node, 128GB+ RAM)\": {\n",
    "        \"pair_buffer_reserve\": 500000,\n",
    "        \"channel_buffer\": 100000,\n",
    "        \"max_seqs\": 128,\n",
    "        \"threads\": 16,\n",
    "        \"estimated_peak\": \"~8GB\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Memory configuration profiles:\\n\")\n",
    "for profile, config in memory_configs.items():\n",
    "    print(f\"{profile}:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunked-header",
   "metadata": {},
   "source": [
    "### Chunked Processing for Very Large Datasets\n",
    "\n",
    "For datasets too large to process in one pass, split by chromosome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunked-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_script = '''#!/bin/bash\n",
    "# Process BAM chromosome-by-chromosome to reduce memory\n",
    "\n",
    "BAM=$1\n",
    "VCF=$2\n",
    "OUTDIR=$3\n",
    "\n",
    "# Get chromosome list from BAM\n",
    "CHROMS=$(samtools view -H $BAM | grep \"^@SQ\" | cut -f2 | sed 's/SN://')\n",
    "\n",
    "# Process each chromosome separately\n",
    "for CHR in $CHROMS; do\n",
    "    echo \"Processing $CHR...\"\n",
    "    \n",
    "    # Extract chromosome\n",
    "    samtools view -b $BAM $CHR > ${OUTDIR}/${CHR}.bam\n",
    "    samtools index ${OUTDIR}/${CHR}.bam\n",
    "    \n",
    "    # Run WASP2 on subset\n",
    "    wasp2-map make-reads \\\n",
    "        --bam ${OUTDIR}/${CHR}.bam \\\n",
    "        --vcf $VCF \\\n",
    "        --region $CHR \\\n",
    "        --out_dir ${OUTDIR}/${CHR}/\n",
    "    \n",
    "    # Clean up intermediate file\n",
    "    rm ${OUTDIR}/${CHR}.bam*\n",
    "done\n",
    "\n",
    "# Merge results\n",
    "cat ${OUTDIR}/*/counts.tsv > ${OUTDIR}/all_counts.tsv\n",
    "'''\n",
    "\n",
    "print(\"Chunked processing script:\")\n",
    "print(chunked_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "profiling-header",
   "metadata": {},
   "source": [
    "### Memory Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "profiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiling_example = '''# Profile memory usage with memory_profiler\n",
    "pip install memory_profiler\n",
    "\n",
    "# Run with memory profiling\n",
    "mprof run wasp2-map make-reads --bam input.bam --vcf variants.vcf.gz\n",
    "\n",
    "# View memory plot\n",
    "mprof plot\n",
    "\n",
    "# Or use peak memory reporting\n",
    "/usr/bin/time -v wasp2-map make-reads --bam input.bam --vcf variants.vcf.gz 2>&1 | \\\n",
    "    grep \"Maximum resident set size\"\n",
    "'''\n",
    "\n",
    "print(\"Memory profiling commands:\")\n",
    "print(profiling_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "---\n\n## Summary\n\n**Key takeaways:**\n\n1. **Format choice matters**: Use BCF for production, VCF for debugging\n2. **Leverage Rust acceleration**: 5-61x speedups available via `wasp2_rust` module\n3. **Scale to HPC**: Use SLURM scripts or Nextflow for cluster deployment\n4. **Tune memory**: Adjust buffer sizes based on available RAM\n5. **Validate inputs**: Check files exist and are indexed before running\n6. **Handle errors gracefully**: Use try/except and check return codes\n\n**Further reading:**\n- [WASP2 Benchmarking Framework](../benchmarking/README.md)\n- [Nextflow Modules Documentation](../pipelines/nf-modules/README.md)\n- [Rust Extension Source](../rust/src/lib.rs)"
  },
  {
   "cell_type": "markdown",
   "id": "w81n0u527ji",
   "source": "---\n\n## 5. Input Validation & Troubleshooting\n\nProper input validation prevents cryptic errors and wasted compute time.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "104lw55838sf",
   "source": "### Input Validation Checklist\n\nBefore running WASP2, verify these requirements:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3aawo95fp6f",
   "source": "def validate_wasp2_inputs(bam_path: str, vcf_path: str, sample: str = None) -> dict:\n    \"\"\"\n    Comprehensive input validation for WASP2 pipelines.\n    \n    Returns a dict with validation results and any errors found.\n    \"\"\"\n    from pathlib import Path\n    import subprocess\n    \n    results = {\n        \"valid\": True,\n        \"errors\": [],\n        \"warnings\": [],\n        \"info\": {}\n    }\n    \n    bam = Path(bam_path)\n    vcf = Path(vcf_path)\n    \n    # 1. Check BAM file exists and get size\n    if not bam.exists():\n        results[\"errors\"].append(f\"BAM file not found: {bam}\")\n        results[\"valid\"] = False\n    elif not bam.is_file():\n        results[\"errors\"].append(f\"BAM path is not a file: {bam}\")\n        results[\"valid\"] = False\n    else:\n        try:\n            results[\"info\"][\"bam_size_mb\"] = bam.stat().st_size / (1024 * 1024)\n        except OSError as e:\n            results[\"warnings\"].append(f\"Could not stat BAM file: {e}\")\n        \n        # Check BAM index\n        bai_path = Path(str(bam) + \".bai\")\n        alt_bai = bam.with_suffix(\".bai\")\n        if not bai_path.exists() and not alt_bai.exists():\n            results[\"errors\"].append(\n                f\"BAM index not found. Create with: samtools index {bam}\"\n            )\n            results[\"valid\"] = False\n    \n    # 2. Check VCF file exists and get size\n    if not vcf.exists():\n        results[\"errors\"].append(f\"VCF file not found: {vcf}\")\n        results[\"valid\"] = False\n    elif not vcf.is_file():\n        results[\"errors\"].append(f\"VCF path is not a file: {vcf}\")\n        results[\"valid\"] = False\n    else:\n        try:\n            results[\"info\"][\"vcf_size_mb\"] = vcf.stat().st_size / (1024 * 1024)\n        except OSError as e:\n            results[\"warnings\"].append(f\"Could not stat VCF file: {e}\")\n        \n        # Check for tabix index if compressed\n        if str(vcf).endswith('.gz'):\n            tbi_path = Path(str(vcf) + \".tbi\")\n            csi_path = Path(str(vcf) + \".csi\")\n            if not tbi_path.exists() and not csi_path.exists():\n                results[\"warnings\"].append(\n                    f\"VCF index not found. Consider: tabix -p vcf {vcf}\"\n                )\n    \n    # 3. Validate sample name exists in VCF (if provided)\n    if sample and vcf.exists():\n        try:\n            cmd = [\"bcftools\", \"query\", \"-l\", str(vcf)]\n            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)\n            if result.returncode == 0:\n                samples_in_vcf = result.stdout.strip().split('\\n')\n                results[\"info\"][\"vcf_samples\"] = samples_in_vcf\n                if sample not in samples_in_vcf:\n                    results[\"errors\"].append(\n                        f\"Sample '{sample}' not found in VCF. \"\n                        f\"Available: {', '.join(samples_in_vcf[:5])}\"\n                    )\n                    results[\"valid\"] = False\n        except FileNotFoundError:\n            results[\"warnings\"].append(\"bcftools not available for sample validation\")\n        except subprocess.TimeoutExpired:\n            results[\"warnings\"].append(\"bcftools timed out during sample check\")\n        except Exception as e:\n            results[\"warnings\"].append(f\"Error validating sample: {type(e).__name__}: {e}\")\n    \n    return results\n\n# Example usage\nprint(\"Input Validation Example:\")\nprint(\"-\" * 40)\n\nvalidation = validate_wasp2_inputs(\n    str(bam_file),\n    str(vcf_file),\n    sample=\"sample1\"\n)\n\nif validation[\"valid\"]:\n    print(\"All inputs are valid!\")\nelse:\n    print(\"Validation FAILED:\")\n    \nfor error in validation[\"errors\"]:\n    print(f\"  ERROR: {error}\")\nfor warning in validation[\"warnings\"]:\n    print(f\"  WARNING: {warning}\")\n    \nprint(f\"\\nInput info:\")\nfor key, value in validation[\"info\"].items():\n    if isinstance(value, float):\n        print(f\"  {key}: {value:.2f}\")\n    elif isinstance(value, list) and len(value) > 3:\n        print(f\"  {key}: {value[:3]} ... ({len(value)} total)\")\n    else:\n        print(f\"  {key}: {value}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "m08dv9r0izj",
   "source": "### Common Errors and Solutions\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| `RuntimeError: BAM index not found` | Missing .bai file | Run `samtools index input.bam` |\n| `RuntimeError: BCF format not supported` | BCF input to Rust parser | Convert to VCF.gz: `bcftools view -Oz input.bcf > input.vcf.gz` |\n| `Sample 'X' not found in VCF` | Typo or wrong VCF | Check samples with `bcftools query -l input.vcf.gz` |\n| `MemoryError` or OOM killed | Insufficient RAM | Reduce `pair_buffer_reserve` and `channel_buffer`, or use chunked processing |\n| `Too many open files` | ulimit too low | Run `ulimit -n 65536` before job |\n| `Rust extension not available` | Extension not built | Run `maturin develop --release -m rust/Cargo.toml` |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "fneppl9hsmm",
   "source": "### Debugging Tips for HPC Environments\n\n```bash\n# 1. Check if Rust extension loads correctly\npython -c \"import wasp2_rust; print('OK')\"\n\n# 2. Verify BAM is sorted and indexed\nsamtools quickcheck input.bam && echo \"BAM OK\" || echo \"BAM corrupt\"\nsamtools idxstats input.bam | head -5\n\n# 3. Check VCF is valid\nbcftools stats input.vcf.gz | head -20\n\n# 4. Monitor memory during run\nwatch -n 5 'ps -o pid,rss,vsz,comm -p $(pgrep -f wasp2)'\n\n# 5. Check for stalled processes\nstrace -p <PID> -e trace=read,write 2>&1 | head -20\n\n# 6. Verify output files are being written\nwatch -n 10 'ls -lh output_dir/*.fq.gz 2>/dev/null'\n```",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}